---
title: "A-SA24204129-Homework"
date: "2024-12-06"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A-SA24204129-Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 作业2024-09-09

## ***Example 1：主成分分析***

$\hspace{1.5em}$在许多实际应用中，收集到的海量数据往往价值密度较低，包含大量噪声和冗余，高维的数据暗藏着低维的结构。基于此，降维技术应运而生，其中最常用的就是**主成分分析(principal component analysis, PCA)**。

$\hspace{1.5em}$主成分是将自变量进行新的线性组合，生成新的信息表达方式，在尽量不损失原始数据信息的情形下，将数据信息进行有效地转换与压缩。同时，主成分互不相关的性质对解释以及之后的统计分析都有帮助。

### **理论部分**

$\hspace{1.5em}$假设研究数据包括样本和变量的数目分别为$n$和$p$。$p$个变量按顺序表示为$X_1,X_2,…,X_p$，由这$p$个变量构成的随机向量可表示为$\textbf{X}=(X_1,X_2,…,X_p)'$.$\mu$表示$\textbf{X}$的均值，$\Sigma$表示$\textbf{X}$的协方差矩阵。Y表示提取到的主成分，则主成分由原始变量线性表示，如下式：
$$
\begin{align}\left\{\begin{aligned}
\begin{matrix}
Y_1=c_{11}X_1+C_{21}X_2+…+c_{p1}X_p=\textbf{c}_1^TX\\
Y_2=c_{12}X_1+C_{22}X_2+…+c_{p2}X_p=\textbf{c}_2^TX\\
……\\
Y_p=c_{11}X_1+C_{21}X_2+…+c_{p1}X_p=\textbf{c}_1^TX\\
\end{matrix}
\end{aligned}\right.\end{align}
$$

$\hspace{1.5em}$在$Y_i=c_i'\textbf{X}$的方差尽可能大且各$Y_i$之间相互独立的前提下，求解出的主成分会损失原始变量较少的信息。$Y_i$的方差可表示为$Var(Y_i)=Var(c_i'\textbf{X})=c_i'\Sigma c_i$，对于任意常数$k$，有$Var(k c_i'\textbf{X})=k^2 c_i'\Sigma c_i$。需要限制$c_i$的值，使$Var(Y_i)$不能任意增大，所以在进行线性变换时要满足一定的约束条件：

$\hspace{1.5em}$(1)$c_i'c_i=1,(i=1,2,…,p)$;

$\hspace{1.5em}$(2)主成分$Y_i$与$Y_j$之间互不相关($i\neq j, i,j=1,2,…,p$);

$\hspace{1.5em}$(3)在$\textbf{X}$的所有满足约束条件(1)的线性组合中，方差最大的是$Y_1$；在与$Y_1$不相关的所有$\textbf{X}$的线性组合中，方差最大的是$Y_2$；以此类推，在与$Y_1,Y_2,…,Y_p$都不相关的所有$\textbf{X}$的线性组合中，方差最大的是$Y_p$。

### **R软件实现**

$\hspace{1.5em}$下面是主成分分析经典的一个数值模拟方法(Zou等，2006),我们将基于这个模拟数据进行操作。${V_1},{V_2},{V_3}$分别是三个潜在主成分，分别满足 ${V_1}\sim N(0,290)$,${V_2}\sim N(0,300)$,${V_3}=-0.3V_1+0.925V_2+e,e\sim N(0,1)$,且$V_1,V_2$与$e$相互独立。设有10个变量，其构造方式如下：
$$
\begin{align}
X_i&=V_1+e_i^1,e_i^1\sim N(0,1),i=1,2,3,4;
\\
X_i&=V_2+e_i^2,e_i^2\sim N(0,1),i=5,6,7,8;
\\
X_i&=V_3+e_i^3,e_i^3\sim N(0,1),i=9,10.
\end{align}
$$
其中 {$e_i^j,i=1,2,…,10,j=1,2,3$} 相互独立。R语言运行结果如下：
```{r echo=FALSE}
####——————【【数据模拟】】——————
# 种子设置
set.seed(6543)

# 设置参数
n <- 1000000   # 样本数
p <- 10    # 变量数

# 生成V_1和V_2的随机数
V_1 <- rnorm(n, mean = 0, sd = sqrt(290))
V_2 <- rnorm(n, mean = 0, sd = sqrt(300))

# 计算V_3
e <- rnorm(n)
V_3 <- -0.3 * V_1 + 0.925 * V_2 + e

# 至此,三个隐藏因子构造完毕

# 初始化矩阵X，使用零矩阵确保结构正确
X <- matrix(0, nrow = n, ncol = p)  # 确保是一个n行p列的零矩阵

# 构造X矩阵的前4列
for (i in 1:4) {
  e1 <- rnorm(n)  # 随机误差
  X[, i] <- V_1 + e1  # 确保每列的长度一致
}

# 构造X矩阵的第5至8列
for (i in 5:8) {
  e2 <- rnorm(n)  # 随机误差
  X[, i] <- V_2 + e2  # 确保每列的长度一致
}

# 构造X矩阵的第9和第10列
for (i in 9:10) {
  e3 <- rnorm(n)  # 随机误差
  X[, i] <- V_3 + e3  # 确保每列的长度一致
}

# 检查矩阵 X 的维度
dim(X)  # 确保它是 n x p 的矩阵

# 对X进行标准化
X_standardized <- scale(X)   # 对X标准化-零均值&单位方差
# 或者按列逐一标准化:
# X_standardized <- apply(X, 2, scale)

####——————【【PCA】】——————
# 进行主成分分析（PCA）
pca_res <- prcomp(X_standardized, scale. = TRUE)  

# 载荷（loadings）
pca_loadings <- pca_res$rotation    

# 方差贡献率（%）
pca_variance_contrib <- pca_res$sdev^2 / sum(pca_res$sdev^2) * 100  

# 累积贡献率（%）
pca_cum_contrib <- cumsum(pca_variance_contrib)  

# 【输出结果】
combined_mat <- rbind(pca_loadings[,1:3], pca_variance_contrib[1:3], pca_cum_contrib[1:3])    

# 转化为表格
table <- as.data.frame(combined_mat)

# 给行加名字
rownames(table) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','方差贡献率(%)','累积贡献率(%)')

# 输出表格
knitr::kable(table, format = "markdown", digits = 3)

```
$\hspace{1.5em}$PCA方法提取前三个主成分累积方差贡献率为***`r round(pca_cum_contrib[2],2)`%***，结果符合模拟数据的设定。但PCA无法识别其真实稀疏结构,这是由于主成分的计算依托于全部原始变量，获得的每个PC是所有变量的线性组合。为了达到降维和可解释性之间的良好平衡，将稀疏性这一简单性假设引入PCA中，从而改善主成分的不足。在这里不再讨论。

------

## ***Example 2：寻找组间差异——t检验***

$\hspace{1.5em}$在研究中，我们最常关注的问题莫过于处理组与对照组是否存在了显著不同。就两组间比较而言，t检验是常见的分析方法之一。

$\hspace{1.5em}$鸢尾花(Iris)数据集是机器学习领域中最经典的数据集之一。它由三种不同品种的鸢尾花的测量数据组成：山鸢尾(setosa)、变色鸢尾(versicolor)和维吉尼亚鸢尾(virginica)。该数据集包含150个样本，每行数据包含每个样本的四个特征：花萼长度(sepal length)、花萼宽度(sepal width)、花瓣长度(petal length)、花瓣宽度(petal width)。

$\hspace{1.5em}$接下来,使用 iris 数据集中的 Sepal.Length 进行 t 检验，比较不同种类的 Sepal.Length 是否有显著差异。

### **正态性假设检验**

$\hspace{1.5em}$t检验的一个重要前提就是数据必须符合*正态分布*。因此在执行t检验之前必须验证数据分布的正态性。若数据不符合正态分布，则t检验将无法适用于该数据。首先绘制QQ图查看数值分布，如果点大致沿着对角线分布，说明数据可能近似正态分布；偏离对角线可能表明数据不完全符合正态分布。结果如下图所示。

```{r iris, echo=FALSE}
# 提取 Sepal.Length 数据
sepal_length = iris$Sepal.Length

# 正态性检验：绘制 QQ 图
qqnorm(sepal_length, main="QQ Plot of Sepal Length",col="deeppink")
qqline(sepal_length, col="deepskyblue")
```

$\hspace{1.5em}$横坐标是标准的正态分布值，纵坐标是我们数据的值。可以看到大部分的点都离直线很近，大致沿着对角线分布，因此可以认为该数据符合正态性假设。

### **t检验**

$\hspace{1.5em}$接下来对数据进行独立样本的 t 检验，即分别两两比较不同种类的 Sepal.Length。R软件操作得到三组结果如下：

```{r sepal_length, echo=FALSE}
# 因为 Sepal.Length 是一个连续变量，我们可以将其按种类分组进行 t 检验
# 提取每个种类的数据
setosa <- iris$Sepal.Length[iris$Species == "setosa"]
versicolor <- iris$Sepal.Length[iris$Species == "versicolor"]
virginica <- iris$Sepal.Length[iris$Species == "virginica"]

# 独立样本 t 检验：Setosa vs Versicolor
t_test_setosa_versicolor <- t.test(setosa, versicolor, var.equal = TRUE)
print(t_test_setosa_versicolor)

# 独立样本 t 检验：Setosa vs Virginica
t_test_setosa_virginica <- t.test(setosa, virginica, var.equal = TRUE)
print(t_test_setosa_virginica)

# 独立样本 t 检验：Versicolor vs Virginica
t_test_versicolor_virginica <- t.test(versicolor, virginica, var.equal = TRUE)
print(t_test_versicolor_virginica)
```

$\hspace{1.5em}$可以看到，结果的p值均小于显著性水平0.01，这表明两组间的 Sepal.Length 存在显著差异。

### **可视化——箱线图**
$\hspace{1.5em}$箱线图能帮助我们更直观地了解数据的分布情况、中心趋势和离群情况。观察三组的箱体，发现在中位数、四分位数和异常值有着较为明显的差异。
```{r iris sepal_length, echo=FALSE}

boxplot(Sepal.Length ~ Species, data=iris, main="Boxplot of Sepal Length by Species", 
        col=c("darkgray",'gold','deepskyblue'), border=c('deeppink','purple','red'))
        

```

------

## ***Example 3：一元线性回归分析***

$\hspace{1.5em}$一元线性回归分析是一种最常用、最简单的估计。

$\hspace{1.5em}$R中自带的women数据集储存了15个年龄在30~39岁间的女性的身高和体重信息，我们将建立一元线性回归模型，试图通过身高预测体重。

### **模型构建与解读**

$\hspace{1.5em}$。散点图可以判断两个变量之间有无相关关系，并对变量间的关系形态做出大致的描述:

```{r women,echo=FALSE}
plot(women$height, women$weight,
xlab = "Height (in inchs)", ylab = "Weight (in pounds)",col='deeppink',pch=19)
mo=lm(women$weight ~ women$height)
a=mo$coefficients[1]
b=mo$coefficients[2]
mo_result=summary(mo)
squ_h= I((women$height)^2)

```

$\hspace{1.5em}$可以看到散点分布呈现线性规律，说明适合构建线性回归方程。

$\hspace{1.5em}$接着构建模型,用summary函数查看拟合模型结果如下:

```{r mo_result,echo=FALSE}
mo_result
```

$\hspace{1.5em}$可以看到，拟合结果$adj.R^2$达到`r round(summary(mo)$adj.r.squared,4)`，这表明模型的解释性非常好;同时F检验的p-value为`r round(summary(mo)$fstatistic,4)`远远小于0.05，说明模型通过了F检验；截距项和系数也都通过了t检验；height的系数为正，这表明随着身高的增长，体重也增长，符合客观事实。得到回归方程如下：

$$
weight=`r a`+`r b`*height
$$

进一步用图像对模型进行回归诊断：

```{r mo , fig.width=7, fig.height=5,echo=FALSE}
par(mfrow=c(2,2)) 
plot(mo)
par(mfrow=c(1,1)) # 恢复设置
```

1）左上图为**残差与拟合图**，理论上散点应该散乱的分布在横线两侧，但是此图明显有一个曲线关系，说明我们的模型需要加入一个二次项。

2）右上图是一个**正态Q-Q图**，用于检验因变量的正态分布性，若服从，则散点应分布在一条直线上。可以看出该图是符合这一条件的，表明满足正态性假设。

3）左下图为**齐方差检验**，若满足其方差，则散点在水平线周围随机分布，此图满足齐方差检验。

4）右下图是**独立性检验**，即一个样本是否会影响另一个样本，我们的样本数据并不存在这样的问题。


### **模型修正**

$\hspace{1.5em}$根据以上分析，我们对模型引入一个二次项，得到结果如下：

```{r women squ_h, echo=FALSE}
mo_1=lm(women$weight ~ women$height + squ_h)
summary(mo_1)
plot(women$height, women$weight,
xlab = "Height (in inchs)", ylab = "Weight (in pounds)",col='deeppink')
lines(women$height, col='deepskyblue',fitted(mo_1))
```

$\hspace{1.5em}$得到的$adj.R^2$提高到`r round(summary(mo_1)$adj.r.squared,4)`,对比来看，模型拟合效果得到提升。

------

# 作业2024-09-14

## ***习题3.4***

### 答：

$\hspace{1.5em}$**Rayleigh 分布**（瑞利分布）是概率论中的一种概率分布，其概率密度函数如下：
$$
f(x)=\frac{x}{\sigma^2}e^\frac{-x^2}{2\sigma^2}
$$
其中,$\sigma$是分布的尺度参数。其分布函数为：
$$
F(x)=1-e^\frac{-x^2}{2}
$$
$\hspace{1.5em}$下面将选取$\sigma$分别为**0.5、1、2、4**，从瑞利分布中生成随机样本。并利用**直方图**检验生成的样本的模式是否接近理论模式。由于瑞利分布为连续型分布，这里我们选用**逆变换法**。即首先生成一个0到1上的均匀分布U,再利用逆变换$X=F_X^{-1}(U)$生成随机数X。最终得到结果如下图所示。

```{r echo=TRUE,fig.width=7, fig.height=5}
# 设置随机种子 可复现
set.seed(1234)

# 定义生成瑞利分布样本的函数
rayleigh_samples = function(sigma, n) {
  U = runif(n) #生成[0,1]
  samples = sigma * sqrt(-2 * log(U)) #逆变换
  return(samples)
}

# 选取一系列Rayleigh分布的参数sigma
sigmas=c(0.5,1,2,4)
n=8000  # 样本数量

# 循环生成样本并绘制直方图
par(mfrow = c(2, 2))  # 设置图形布局2×2
for (sigma in sigmas) 
  {
  samples = rayleigh_samples(sigma, n)
  # 绘制直方图
  hist(samples, breaks = 30, main = paste("Rayleigh Distribution (sigma =", sigma, ")"), 
       xlab = "Value", col = "lightblue", border = "black", prob = TRUE)
  # 添加理论密度函数曲线
  curve((x / sigma^2) * exp(-x^2 / (2 * sigma^2)), add = TRUE, col = "deeppink", lwd = 2)

  # 添加众数线
  abline(v = sigma, col = "blue", lwd = 2, lty = 2)  # 理论众数
  legend("topright", legend = c('Mode', "Density"), col = c("blue", "deeppink"), lwd = 2)
}
par(mfrow = c(1, 1)) # 还原图形布局
```

$\hspace{1.5em}$从上图我们可以看到，模拟的分布直方图和概率密度函数形状接近，这表明我们的模拟是可以接受的。同时通过几张图的对比可以看出，尺度参数 σ 的值越大，分布就越宽。

------

## ***习题3.11***

### 答：

```{r echo=TRUE,fig.width=7, fig.height=5}
set.seed(1234)
# 创建一个函数模拟并生成直方图
mix_hist=function(n,p1, co='lightblue') 
  #参数n为生成随机样本数,p1为N(0,1)的混合概率,co为绘制颜色
{
  p=rbinom(n,1,prob = p1)
  #生成两个正态分布随机数 N(0,1)和N(3,1)
  x1=rnorm(n) 
  x2=rnorm(n,3,1)
  x=p*x1+(1-p)*x2 #混合
  #绘制直方图
  title=paste('p1 = ',p1)
  hist(x,probability = T,main = title,col = co)
  #理论密度函数
  densityplot=function(x)
    {
      p1*dnorm(x)+(1-p1)*dnorm(x,3,1)
  }
  y=seq(-3,6,0.1)
  lines(y,densityplot(y),col='deeppink',lwd=2)
} 

mix_hist(50000,0.75)#p1=0.75的情况

par(mfrow = c(3, 3))  # 设置图形布局3×3
for (p1 in seq(0.1,0.9,0.1)) #p1取值为0.1,0.2,……,0.9
  {
  mix_hist(50000,p1,'gray')
  }
par(mfrow = c(1, 1)) # 还原图形布局
```
       
$\hspace{1.5em}$由结果图看出，对于$p1$的取值，当$p1$值接近0.5时，分布更倾向于双峰。而更高的值，如 $p1>$ 0.75，导致以第一个分量为中心的单峰分布；更低的值，如 $p1<$ 0.3，导致以第二个分量为中心的单峰分布。

------

## ***习题3.20***

### 答：

```{r echo=TRUE}

set.seed(1234)
# 函数一：复合泊松-伽马过程模拟函数
com_poisson_gamma = function(lambda_rate, alpha, beta, T, num) 
  {
  X_T = numeric(num)  # 初始化结果向量
  
  for (i in 1:num) {
    # 生成泊松过程中的事件数量
    N_T = rpois(1, lambda_rate * T)
    # 生成伽马分布的随机变量并求和
    if (N_T > 0) 
      {
      Y = rgamma(N_T, shape = alpha, rate = beta)
      X_T[i] = sum(Y)} 
    else {
      X_T[i] = 0}
  }
  return(X_T)
}

# 函数2：估计平均值和方差
mean_var = function(X_T, lambda_rate, alpha, beta, T) {  
  emp_mean = mean(X_T)  
  emp_var = var(X_T)  
  theo_mean = lambda_rate * T * (alpha / beta)  
  theo_var = lambda_rate * T * ((alpha * (alpha + 1)) / beta^2 - (alpha / beta)^2)  
  return(list(emp_mean = emp_mean,  
              emp_var = emp_var,  
              theo_mean = theo_mean,  
              theo_var = theo_var))  
} 

# 参数设置
lambda_rate = 2.0  # 泊松率参数
alpha = 2.0        # 伽马分布形状参数
beta = 1.0         # 伽马分布率参数
T = 10             # 时间
num = 50000        # 模拟次数

# 运行模拟
X_T = com_poisson_gamma(lambda_rate, alpha, beta, T, num)

# 估计平均值和方差
results = mean_var(X_T, lambda_rate, alpha, beta, T)
# 计算实验的平均值和标准差
emp_mean = results$emp_mean
emp_sd = sqrt(results$emp_var)


# 创建一个数据框以便输出为表格
result_table = data.frame(
  Statistic = c("实验平均值", "理论平均值", "实验方差", "理论方差"),
  Value = c(results$emp_mean, results$theo_mean, results$emp_var, results$theo_var)
)
knitr::kable(result_table, caption = "复合泊松-伽马过程的模拟统计结果")
```
       
$\hspace{1.5em}$输出结果中，实验均值和理论均值接近，实验方差和理论方差也接近，这从**数值上表明模拟的准确性较高**。直方图显示了模拟数据的实际分布情况，而KDE曲线则是对这种分布的一种平滑近似。如果两者的形状近似，那么这表明模拟数据在整体上呈现出一种相对稳定的分布模式，没有出现异常或不一致的偏差，暗示了模拟过程的合理性。通过可视化，可以快速了解**模拟数据的分布情况**。

```{r X_T ,echo=TRUE,fig.width=7, fig.height=5}
# 绘制直方图
hist(X_T, breaks = 50, main = "Histogram of X(10)", 
     xlab = "X(10) values", col = "lightblue", probability = TRUE)
# 绘制核密度估计曲线  
density_curve = density(X_T, adjust = 1)  
lines(density_curve, col = "deeppink", lwd = 2)  

text(x = max(density_curve$x) * 0.9, y = max(density_curve$y) * 1.1,   
     labels = "Kernel Density Estimate", pos = 4, col = "red")  
```
  
------

# 作业2024-09-23

## ***习题5.4***
$\hspace{1.5em}$编写一个函数来计算Beta(3,3)的蒙特卡罗估计数，并使用该函数来估计x =0.1,0.2，...，0.9的F(x)。将估计值与R中的pbeta函数返回的值进行比较。

### 答：

```{r}
# 加载必要的包
library(ggplot2)
# 设置参数
n = 10000  # 蒙特卡罗样本数量
a = 3      # Beta 分布的第一个形状参数
b = 3      # Beta 分布的第二个形状参数

# Monte Carlo 估计 CDF 的函数
mc.cdf.beta = function(p, n, shape1, shape2) 
  {
  if (p <= 0 || p >= 1) return(0)  # 确保 p 在 (0, 1) 范围内
  samples = rbeta(n, shape1, shape2)  # 生成 n 个 Beta 分布的样本
  return(mean(samples <= p))  # 计算小于等于 p 的比例
}

# 要估计的 x 值
x_values = seq(0.1, 0.9, by = 0.1)

# 计算 Monte Carlo 估计和精确 CDF
mc_estimates = sapply(x_values, function(x) mc.cdf.beta(x, n, a, b))
exact_values = sapply(x_values, function(x) pbeta(x, a, b))

# 输出结果
result = data.frame(
  x = x_values,
  Monte_Carlo_Estimate = mc_estimates,
  Exact_Value = exact_values
)
knitr::kable(result, caption = "Monte Carlo 估计和精确 CDF比较表",digits = 3)

# 绘制对比图
ggplot(result, aes(x = x)) +
  geom_line(aes(y = Monte_Carlo_Estimate, color = "蒙特卡罗估计"), linewidth = 1.2) +
  geom_point(aes(y = Monte_Carlo_Estimate, color = "蒙特卡罗估计"), size = 3) +
  geom_line(aes(y = Exact_Value, color = "精确值"), linewidth = 1.2, linetype = "dashed") +
  geom_point(aes(y = Exact_Value, color = "精确值"), size = 3) +
  labs(title = "Beta(3, 3) CDF 蒙特卡罗估计与精确值对比",
       x = "x 值",
       y = "CDF 值") +
  scale_color_manual(values = c("蒙特卡罗估计" = "deepskyblue", "精确值" = "deeppink")) +
  theme_minimal(base_size = 15) +  # 增加基础字体大小
  theme(
    legend.title = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),  # 标题居中并加粗
    axis.title.x = element_text(face = "bold"),  # X轴标题加粗
    axis.title.y = element_text(face = "bold")   # Y轴标题加粗
  )


```

$\hspace{1.5em}$我们通过**数值**和**图像可视化**形式分别进行对比，得到具体结果如上表和上图所示。可以看到，模拟结果很好，与pbeta函数返回的值相差不大。

------

## ***习题5.9***

### 答:

#### Rayleigh分布
$\hspace{1.5em}$Rayleigh分布的概率密度函数为：

$$
f(x)=\frac{x}{\sigma^2}e^\frac{-x^2}{2\sigma^2},x\geq0,\sigma>0
$$
使用逆变换法生成随机样本。

#### 对立变量

$\hspace{1.5em}$对立变量技术涉及生成两个负相关的变量$X_1$和$X_2$。设U是均匀分布在[0,1]的随机变量，使用U生成$X_1$，使用1-U生成$X_2$

```{r}
# 函数：生成Rayleigh分布样本
rayleigh_sample <- function(sigma, n_samples) {
  U <- runif(n_samples)  # 生成均匀分布随机数
  X1 <- sigma * sqrt(-2 * log(U))       # 使用U生成X1
  X2 <- sigma * sqrt(-2 * log(1 - U))   # 使用(1-U)生成X2
  return(list(X1 = X1, X2 = X2))
}

# 函数：计算方差减少并绘制图形
variance_reduction_and_plot <- function(sigma, n_samples) 
  {
  samples <- rayleigh_sample(sigma, n_samples)
  X1 <- samples$X1
  X2 <- samples$X2
  
  # 计算对立变量的平均
  X_plus_antithetic <- (X1 + X2) / 2
  
  # 独立变量生成
  independent_X1 <- sigma * sqrt(-2 * log(runif(n_samples)))
  independent_X2 <- sigma * sqrt(-2 * log(runif(n_samples)))
  X_plus_independent <- (independent_X1 + independent_X2) / 2
  
  # 计算方差
  var_antithetic <- var(X_plus_antithetic)
  var_independent <- var(X_plus_independent)
  
  # 计算方差减少百分比
  percent_reduction <- (var_independent - var_antithetic) / var_independent * 100
  
  # 绘制直方图
  par(mfrow = c(1, 2))  # 设置图形区域为1行2列
  
  hist(X_plus_independent, breaks = 30, col = 'deepskyblue', 
       main = '独立变量生成的Rayleigh样本', 
       xlab = '样本值', 
       ylab = '频率', 
       xlim = c(0, max(X_plus_independent, X_plus_antithetic)),
       probability = TRUE)
  
  hist(X_plus_antithetic, breaks = 30, col = 'deeppink', 
       main = '对立变量生成的Rayleigh样本', 
       xlab = '样本值', 
       ylab = '频率', 
       xlim = c(0, max(X_plus_independent, X_plus_antithetic)),
       probability = TRUE)

  legend("topright", legend = c("独立变量", "对立变量"), 
         fill = c("deepskyblue", "deeppink"))
  
  return(percent_reduction)
}

# 示例使用
sigma <- 1.0  # 参数
n_samples <- 10000  # 样本数量
reduction <- variance_reduction_and_plot(sigma, n_samples)
title(main = paste("\nσ =", sigma), outer = TRUE)
cat(sprintf("方差减少百分比: %.2f%%\n", reduction))
```
------

## ***习题5.13***

### 答：

$\hspace{1.5em}$对于重要性分布$f_1$,由于Gamma分布在 (1,$\infty$)上有较好的支持，因此选择参数为$\alpha$=3,$\beta$=1的Gamma分布，即：
$$
f_1(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} 
= \frac{1^3}{2} x^{3-1} e^{-x} = \frac{1}{2} x^2 e^{-x}
$$

$\hspace{1.5em}$对重要性分布 $f_2$，我们可以选择正态分布，例如 $N(\mu=2, \sigma^2=1)$，在 (1,$\infty$) 区间上也是合适的。正态分布的概率密度函数为：
$$
f_2(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-2)^2}{2}}
$$

```{R}
# 加载必要的库
library(ggplot2)
# 设置随机种子以确保结果可重复
set.seed(1234)
# 定义目标分布 g(x)
g <- function(x) {
  (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
}
# 定义重要性分布 f1 (Gamma 分布) 和 f2 (正态分布)
f1 <- function(x) {
  (x^2 * exp(-x)) / 2 # Gamma(3, 1) 的 PDF
}
f2 <- function(x) {
  dnorm(x, mean = 2, sd = 1) # N(2, 1) 的 PDF
}
# 重要性采样函数
importance_sampling <- function(g, f, n) {
  samples <- numeric(n)
  if (identical(f, f1)) {
    samples <- rgamma(n, shape = 3, rate = 1)
  } else if (identical(f, f2)) {
    samples <- rnorm(n, mean = 2, sd = 1)
  }
  # 计算权重
  weights <- g(samples) / sapply(samples, f)
  # 计算估计值和方差
  estimate <- mean(weights)
  variance <- var(weights) / n
  return(list(estimate = estimate, variance = variance, samples = samples))
}

# 执行重要性采样
n_samples <- 10000
result_f1 <- importance_sampling(g, f1, n_samples)
result_f2 <- importance_sampling(g, f2, n_samples)

results_df <- data.frame(
  Distribution = c("Gamma", "Normal"),
  Estimate = c(result_f1$estimate, result_f2$estimate),
  Variance = c(result_f1$variance, result_f2$variance)
)

# 输出结果为表格
knitr::kable(results_df, caption = "重要性采样结果")

# 可视化目标分布和重要性分布
x_vals <- seq(0, 6, length.out = 100)
target_density <- g(x_vals)
gamma_density <- f1(x_vals)
normal_density <- f2(x_vals)

# 创建数据框用于 ggplot
data_plot <- data.frame(
  x = rep(x_vals, 3),
  density = c(target_density, gamma_density, normal_density),
  distribution = rep(c("Target (g)", "Gamma (f1)", "Normal (f2)"), each = length(x_vals))
)

# 绘制图像
ggplot(data_plot, aes(x = x, y = density, color = distribution)) +
  geom_line(linewidth = 1.2) +  
  labs(title = "Comparison of Target and Importance Distributions",
       x = "x", y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("deepskyblue", "deeppink", "darkolivegreen3")) +
  theme(legend.title = element_blank())
```

#### 结果解释：

$\hspace{1.5em}$使用Gamma分布作为重要性分布时，估计值为0.5049；使用正态分布作为重要性分布时，估计值为0.7874。Gamma分布的方差为2.4135e-05，表明其估计值的稳定性较高；正态分布的方差为 0.0048，相对较大，表明其估计值波动较大。这可能是因为正态分布是对称的，可能不够灵活以适应某些数据的偏态分布。而Gamma 分布具有较强的右偏特征，适用于非负且具有较大变异性的数据，通过选择合适的参数可以使得其在关注的区域内有较高的密度，从而提供更稳定的估计值。

------

# 作业2024-10-14

## ***多重假设检验——Bonferroni与B-H方法的模拟比较***

$\hspace{1.5em}$设在N=1000个假设中,950个是零假设,50个是备择假设。任何**零假设**下的p值都是**均匀分布**的(**runif**)，而任何**备择假设**下的p值都遵循**参数为0.1和1的贝塔分布**(**rbeta**)。

$\hspace{1.5em}$分别获取**Bonferroni调整**和**B-H调整**后的p值。进行m=10000次模拟重复，计算两种调整方法中每种方法在名义水平$\alpha$=0.1下的FWER、FDR和TPR。将结果输出到3×2表格中，进一步进行分析。

### 代码实现：

```{r}
rm(list=ls())
#设置种子
set.seed(12345)
# 参数设定
N=1000  # 总假设数
num0=950  # 零假设数
num1=50  # 备择假设数
m=10000  # 模拟次数
alpha=0.1  # 显著性水平

# 初始化存储结果的表格
results=matrix(0, nrow=m, ncol=6)
# 模拟
for (i in 1:m) {
  ## 生成p值
  # 零假设使用均匀分布，备择假设使用贝塔分布
  p_values=c(runif(num0), rbeta(num1, 0.1, 1)) 
  
  ## 校正
  bo_adjusted=p.adjust(p_values, method = "bonferroni")
  bh_adjusted=p.adjust(p_values, method = "BH")
  
  #计算拒绝数目
  rejected_bo=sum(bo_adjusted < alpha)
  rejected_bh=sum(bh_adjusted < alpha)
  
  # 真实拒绝的数目
  true_positive_bo=sum((p_values[(num0 + 1):N] < alpha) & (bo_adjusted < alpha))
  true_positive_bh=sum((p_values[(num0 + 1):N] < alpha) & (bh_adjusted < alpha))
  
  ## FWER
  fwer_bo=rejected_bo / num0
  fwer_bh=rejected_bh / num0
  
  ## FDR
  fdr_bo=ifelse(rejected_bo > 0, (num0 - true_positive_bo) / rejected_bo, 0)
  fdr_bh=ifelse(rejected_bh > 0, (num0 - true_positive_bh) / rejected_bh, 0)
  
  ## TPR
  tpr_bo=true_positive_bo / num1
  tpr_bh=true_positive_bh / num1
  
  # 存储每次模拟的结果
  results[i, ]=c(fwer_bo, fdr_bo, tpr_bo, fwer_bh, fdr_bh, tpr_bh)
}

# 转换为数据框并计算平均值
results1=as.data.frame(results)
colnames(results1)=c("fwer_bo", "fdr_bo", "tpr_bo",
                     "FWER_BH", "FDR_BH", "TPR_BH")

# 计算每列的平均值
results_mean=colMeans(results1)

# 将平均值整理为3×2表格
table=matrix(results_mean, nrow=3, byrow=FALSE)
rownames(table)=c("FWER", "FDR", "TPR")
colnames(table)=c("Bonferroni校正", "B-H校正")
table=round(table, 3)  # 保留三位小数

# 显示结果
knitr::kable(table)

```

### 结果分析：

$\hspace{1.5em}$两种调整方法中每种方法在名义水平$\alpha$=0.1下的FWER、FDR和TPR如上表所示。结果表明，在不同的假设检验中，这两种校正方法的表现有所不同。

#### 1. FWER（家庭错误率）
- **Bonferroni校正**的FWER为 **0.021**，这表示在所有假设检验中，仅有约2.1%的情况下错误拒绝了零假设。
- **B-H校正**的FWER为 **0.033**，略高于Bonferroni校正。这意味着在B-H校正中，错误拒绝的比例相对更高，但仍然保持在可接受的范围内。

**分析**：Bonferroni校正是一种保守的校正方法，能够有效控制FWER，因此其FWER值较低。而B-H校正相对宽松，适合于需要平衡假发现率（FDR）和检出率（TPR）的场景。

#### 2. FDR（假发现率）
- **Bonferroni校正**的FDR为 **48.208**，这个值相对较高，表明在错误拒绝的假设中，有接近48.2%是伪阳性（假阳性）。
- **B-H校正**的FDR为 **30.319**，虽然也是相对较高，但明显低于Bonferroni校正。这意味着B-H校正能够有效减少假阳性的比例。

**分析**：FDR的比较显示出B-H校正在控制假发现率方面的优势。Bonferroni校正虽然FWER较低，但由于其保守性，容易产生较高的假发现率，不适合于假阳性率需要较低的情形。

#### 3. TPR（真正率）
- **Bonferroni校正**的TPR为 **0.399**，表示约39.9%的真实备择假设得到了正确的拒绝。
- **B-H校正**的TPR为 **0.610**，表明在B-H校正下，接近61%的真实备择假设得到了正确的拒绝。

**分析**：B-H校正的TPR高于Bonferroni校正，意味着它在检出真正的效应方面更为有效。这也与FDR的高低相辅相成，较低的FDR伴随较高的TPR显示出B-H校正在进行多重假设检验时的有效性。

#### 总结
- **Bonferroni校正**适用于对假发现率要求严格的场景，其FWER较低，但可能导致TPR降低和FDR增加。
- **B-H校正**则在保持合理的FWER水平的同时，能够提升TPR并降低FDR，适合大规模假设检验中的应用。

---

## ***习题7.4***

$\hspace{1.5em}$依据boot包中提供的空调数据集aircondit，12个观察结果以空调设备故障的间隔时间为单位(3、5、7、18、43、85、91、98、100、130、230、487)。假设故障间隔时间遵循指数模型Exp($\lambda$)。获得危险率$\lambda$的MLE，并使用bootstrap估计估计的偏差和标准误差。

### 代码实现：

```{r}
rm(list=ls())
# 加载空调故障时间数据
library(boot)
hours=aircondit$hours
# 设置Bootstrap参数
set.seed(12345)
B=1e4  # 设置Bootstrap次数为10000次

## 计算lambda的 MLE：n/sum(xi)
lambda_hat=length(hours)/sum(hours)
# 初始化存储向量
boot_lambda_hats=numeric(B)
## 进行Bootstrap抽样
for (b in 1:B) {
  boot_hours=sample(hours, replace=TRUE)
  # 计算Bootstrap样本的 MLE
  boot_lambda_hats[b]=length(boot_hours)/sum(boot_hours)
}
## 计算偏差
boot_lambda_hats_mean=mean(boot_lambda_hats)
bias=boot_lambda_hats_mean - lambda_hat
## 计算标准误差
std_error=sd(boot_lambda_hats)

# 结果输出
results=round(c(MLE=lambda_hat,bias=bias, se.boot=std_error, 
                se.samp=sd(hours)/sqrt(length(hours))), 4)
knitr::kable(results)

# 绘制Bootstrap均值的直方图
hist(boot_lambda_hats, main="Bootstrap Estimates of Lambda", 
     xlab=expression(hat(lambda)), 
     breaks=30, col="lightblue", border="black")
abline(v=lambda_hat, col='red', lwd=2)
```

### 结果分析：

- **偏差(Bias)** 值为 0.0013，说明估计值与真实值之间存在微小的偏差，整体来看估计相对准确。

- **自助法标准误(se.boot)** 通过bootstrap计算，用于评估MLE的不确定性，表示在重复抽样过程中MLE的变异程度。相对较小的值(0.0043)表明bootstrap法的MLE具有一定的稳健性。

- **样本标准误 (se.samp)**用于衡量样本统计量的变异性，这个值相对较大，可能是由于样本数量有限，同时样本中有一些极端值（如 230 和 487），这些值会显著增加标准差，从而导致高的标准误差。

---

## ***习题7.5***

$\hspace{1.5em}$使用standard normal, basic, percentile和BCa方法计算故障之间的平均时间$1/\lambda$的95%引导置信区间。比较这些时间间隔，并解释为什么它们可能会有所不同。

```{r}
library(boot)
# 提取数据
hours=aircondit$hours
# 计算样本均值的倒数
mu=1 / mean(hours)
# 设置随机种子和参数
set.seed(12345)
m=1000  
R=999  

# 初始化置信区间矩阵
ci.norm=ci.basic=ci.perc=ci.bca=matrix(NA, m, 2)

# 定义fun算平均故障时间
mean_failure_time=function(hours, ind) {
  return(1/mean(hours[ind]))  # lambda的倒数为平均故障时间
}

# 执行自助法抽样
for (i in 1:m) {
  boot_results=boot(data = hours, statistic = mean_failure_time, R = R)
  # 计算各类型的置信区间
  ci=boot.ci(boot_results, type = c("norm", "basic", "perc", "bca"))
  ci.norm[i, ]=ci$norm[2:3]
  ci.basic[i, ]=ci$basic[4:5]
  ci.perc[i, ]=ci$percent[4:5]
  ci.bca[i, ]=ci$bca[4:5]
}

# 计算并打印每种置信区间的平均值
result=list(
  norm = round(colMeans(ci.norm), 4),
  basic = round(colMeans(ci.basic), 4),
  perc = round(colMeans(ci.perc), 4),
  bca = round(colMeans(ci.bca), 4)
)
print(result)
# 计算每种置信区间包含mu的比例
cat('norm =', mean(ci.norm[, 1] <= mu & ci.norm[, 2] >= mu), "\n",
    'basic =', mean(ci.basic[, 1] <= mu & ci.basic[, 2] >= mu), "\n",
    'perc =', mean(ci.perc[, 1] <= mu & ci.perc[, 2] >= mu), "\n",
    'BCa =', mean(ci.bca[, 1] <= mu & ci.bca[, 2] >= mu), "\n")
```

### 结果分析：

- **负值的置信区间**:对于norm和basic方法，置信区间的下限为负值，这是不合理的。平均故障时间不应为负值，说明模型可能不适合该数据，或者自助法的分布未能有效捕捉到真实情况。

- **相对合理的置信区间**:perc和bca的置信区间更为合理，且均为正值，表明从该模型推断的平均故障时间是正的。
这些置信区间和它们的差异可以归因于以下几个因素：

1. **计算方法的不同**:
   - **正态置信区间**: 假设样本均值的分布是正态的，使用样本均值和标准误差来构建置信区间。适用于大样本，但在小样本或分布不对称时，可能不太准确。
   - **基本置信区间**: 基于原始样本的重采样，使用样本的原始统计量构建置信区间。这种方法对样本的实际分布有更好的适应性。
   - **百分位置信区间**: 使用重采样分布的百分位数来构建区间，直接依赖于数据的分布，通常能提供更准确的置信区间。
   - **BCa置信区间**: 这是调整后的百分位置信区间，通过引入偏差校正和加速因子来提高置信区间的准确性。

2. **样本特性**:
   - 样本的分布特性可能会影响各自方法的结果。例如，如果样本数据是偏态的，正态置信区间可能不准确，而其他方法（如BCa）则可能更可靠。

3. **置信区间的宽度**:
   - 从输出中可以看到，正态和基本置信区间相对较窄，而百分位和BCa置信区间相对较宽。这可能意味着基于重采样的方法（如百分位和BCa）对数据的变化更敏感，提供了更保守的估计。

4. **包含\(\mu\)的比例**:
   - 所有方法都包含了\(\mu\)，这意味着根据这些方法计算的置信区间都成功地覆盖了样本均值的倒数，表明在这些数据下，样本均值的倒数是一个相对稳定的参数。


------

# 作业2024-10-21

## ***一、习题7.8***

$\hspace{1.5em}$要求使用Jackknife刀切法估计习题7.7中的统计量$\hat{\theta}$的偏误和标准误差，其中$\hat{\theta}$是基于协方差矩阵的主成分分析中第一个特征值所解释的方差比例。

### 代码实现：

```{r}
rm(list=ls()) #清除内存变量
# 导入数据
library(bootstrap)
scores=scor
# 计算 theta_hat
theta_hat=eigen(cov(scores))$values[1]/sum(eigen(cov(scores))$values)
# Jackknife 估计
n=nrow(scores)
theta_jack=sapply(1:n, function(i) {
  eigen(cov(scores[-i, ]))$values[1]/sum(eigen(cov(scores[-i, ]))$values) 
})
# 计算 Jackknife 偏差和标准误差
bias_jack=(n - 1) * (mean(theta_jack) - theta_hat)
se_jack=sqrt((n - 1) * mean((theta_jack - theta_hat)^2))
# 输出结果
results=data.frame(Metric=c("Original", "Bias", "Standard Error"),
  Value=c(round(mean(theta_jack),3), round(bias_jack,3), round(se_jack,3)))
knitr::kable(results, caption="Jackknife方法估计结果")
```

### 结果分析：

$\hspace{1.5em}$Jackknife方法是一种重采样技术，通过系统地从原始样本中移除一个或多个观测值，然后重新计算统计量，以此来估计原始统计量的方差和偏误。根据输出结果：

- **original (0.619)**：是原始样本的估计值，表明在主成分分析中，第一个主成分解释了61.9%的方差，说明样本在该维度上具有较强的解释力。

- **bias (0.001)**：偏误非常小，接近于零，表明使用Jackknife方法后，估计值与原始估计值几乎没有系统性偏差，原始估计较为准确。

- **se (0.050)**：标准误差为0.050，表示虽然有一定的波动，但围绕真实值的变异性相对较小。较小的标准误差意味着估计值更精确，更有信心认为真实值接近0.619。

### 总结

$\hspace{1.5em}$估计值0.619是真实值的良好且精确的估计；偏差很小，表明估计量没有系统性地高估或低估真实值；标准误差很小，表明估计值相对可靠。

---

## ***二、习题7.10***

$\hspace{1.5em}$替换对数-对数模型为三次多项式模型，使用交叉验证选择最佳拟合模型。拟合的模型包括：

1. 线性模型：\( Y=\beta_0 + \beta_1 X + \epsilon \)
2. 二次模型：\( Y=\beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon \)
3. 指数模型：\( \log(Y)=\log(\beta_0) + \beta_1 X + \epsilon \)
4. 三次模型：\( Y=\beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon \)

### 代码实现：

```{r,}
rm(list=ls())
# 加载数据集
library(DAAG)
data(ironslag)
# 定义变量
magnetic=ironslag$magnetic; chemical=ironslag$chemical
n=length(magnetic); e1=e2=e3=e4=numeric(n); a_R21=a_R22=a_R23=a_R24=numeric(n)
```
```{r,echo=FALSE,fig.width=7, fig.height=5}
## 绘制模型图像
par(mfrow=c(2, 2))
a=seq(10, 40, 0.1)
# 线性模型
M1=lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear")
lines(a, predict(M1, newdata=data.frame(chemical=a)), col='red',lwd=2)
# 二次模型
M2=lm(magnetic ~ poly(chemical, 2))
plot(chemical, magnetic, main="Quadratic")
lines(a, predict(M2, newdata=data.frame(chemical=a)), col='deepskyblue',lwd=2)
# 指数模型
M3=lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential")
lines(a, exp(predict(M3, newdata=data.frame(chemical=a))), col='purple',lwd=2)
# 三次模型
M4=lm(magnetic ~ poly(chemical, 3))
plot(chemical, magnetic, main="Cubic")
lines(a, predict(M4, newdata=data.frame(chemical=a)), col='orange', lwd=2)
```

```{r}
## leave-one-out交叉验证
for (k in 1:n) {
  y = magnetic[-k]
  x = chemical[-k]
  
  # 线性模型
  L1 = lm(y ~ x)
  yhat1 = predict(L1, newdata = data.frame(x = chemical[k]))
  e1[k] = magnetic[k] - yhat1
  a_R21[k] = summary(L1)$adj.r.squared
  
  # 二次模型
  L2 = lm(y ~ x + I(x^2))
  yhat2 = predict(L2, newdata = data.frame(x = chemical[k]))
  e2[k] = magnetic[k] - yhat2
  a_R22[k] = summary(L2)$adj.r.squared
  
  # 指数模型
  L3 = lm(log(y) ~ x)
  logyhat3 = predict(L3, newdata = data.frame(x = chemical[k]))
  yhat3 = exp(logyhat3)
  e3[k] = magnetic[k] - yhat3
  a_R23[k] = summary(L3)$adj.r.squared
  
  # 三次模型
  L4 = lm(y ~ x + I(x^2) + I(x^3))
  yhat4 = predict(L4, newdata = data.frame(x = chemical[k]))
  e4[k] = magnetic[k] - yhat4
  a_R24[k] = summary(L4)$adj.r.squared
}

## 计算均方误差和调整后R方(均值)
mse = c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
adj_r_squared = c(mean(a_R21), mean(a_R22), mean(a_R23), mean(a_R24))

# 输出结果
results = data.frame(model_type = c("线性模型", "二次模型", "指数模型", "三次模型"),
                     mes = round(mse, 3),
                     adjR2 = round(adj_r_squared, 3))

knitr::kable(results, caption = "模型评估结果")
```

### 结果分析：

$\hspace{1.5em}$根据输出结果，使用二次多项式回归的预测误差更小且调整后的$R^2$更大，这表明其既有较小的预测误差，又有较高的解释能力。模型残差诊断图像如下：

```{R,echo=FALSE,fig.width=7, fig.height=5}
par(mfrow=c(2, 2))
plot(M2)
```

$\hspace{1.5em}$二次模型拟合看起来总体上是合理的，但存在一些潜在的问题。由Residuals vs Fitted（残差与拟合值图）可以看出，图中残差点随机分布在零线附近，但有一些点偏离较大，可能表明存在轻微的异方差性或有影响点；Q-Q Residuals中的点大体沿着对角线排列，但在尾部有一些偏离；Scale-Location（尺度-位置图）显示残差的绝对值随着拟合值的增加而增加，这表明可能存在异方差性；Residuals vs Leverage（残差与杠杆值图）中一些点的杠杆值较高。
鉴于此，需要进一步的模型诊断。

---

## ***三、习题8.1***

$\hspace{1.5em}$实现Cramér-von Mises两样本检验的置换检验。Cramér-von Mises统计量用于比较两个样本的分布，通常表示为：

$$
W_2 = \frac{mn}{(m+n)^2} \left( \sum_{i=1}^n (F_n(x_i) - G_m(x_i))^2 + \sum_{j=1}^m (F_n(y_j) - G_m(y_j))^2 \right)
$$

其中$F_n$是样本$x$的经验分布函数，$G_m$是样本$y$的经验分布函数。



### 代码实现：

```{r}
rm(list=ls())
# 加载数据
data(chickwts)
x=sort(as.vector(chickwts$weight[chickwts$feed == "soybean"]))
y=sort(as.vector(chickwts$weight[chickwts$feed == "linseed"]))
## 计算Cramér-von Mises统计量
cvm_statistic=function(x, y) {
  n=length(x); m=length(y)
  # 合并样本并计算经验分布
  pooled=c(x, y); Fn=ecdf(x)(pooled); Gm=ecdf(y)(pooled)
  # 计算 Cramér-von Mises 统计量
  W2=((n*m)/((n+m)^2))*(sum((Fn[1:n]-Gm[1:n])^2)+sum((Fn[(n+1):(n+m)]-Gm[(n+1):(n+m)])^2))
  return(W2)
}
t0=cvm_statistic(x, y) #原始统计量
## 置换检验
set.seed(23456)
R=999; z=c(x, y);n=length(x);K=1:length(z);reps=numeric(R)
for (i in 1:R) {
  k=sample(K, size=n, replace=FALSE)
  x1=z[k];  y1=z[-k]
  reps[i]=cvm_statistic(x1, y1)
}
## 计算p值
p=mean(abs(c(t0, reps)) >= abs(t0))
# 输出结果
cat(" Cramér-von Mises 统计量:", round(t0,3), "\n","p 值:", round(p,3), "\n")
```

### 结果分析：

$\hspace{1.5em}$Cramér-von Mises统计量不为零，但p值大于0.05，这意味着我们**没有足够的证据拒绝两个样本具有相同分布的原假设**。直方图展示了置换得到的 Cramér-von Mises 统计量的分布。红色线表示原始统计量的位置，可以看到，大多数置换得到的统计量都小于原始统计量，这与较高的 p 值一致。

```{r,echo=FALSE,fig.width=7, fig.height=5}
hist(reps);abline(v=t0, col="red", lwd=2)
y_position=max(hist(reps, plot=FALSE)$counts)
text(t0+0.15,y_position,labels=paste(round(t0,3)),col="red")
```

---


## ***四、习题8.2***

$\hspace{1.5em}$对双变量斯皮尔曼秩相关检验进行置换检验，并将其与cor.test的p值进行比较。选取习题7.10中用过的ironslag数据集进行分析。

### 代码实现：

```{r}
rm(list=ls())
#加载数据
library(DAAG); data(ironslag)
magnetic=ironslag$magnetic; chemical=ironslag$chemical
# 计算Spearman相关系数和p值
spearman_test=cor.test(magnetic, chemical, method="spearman",exact = FALSE)
spearman_corr=spearman_test$estimate; p_value_cor=spearman_test$p.value
# 置换检验
set.seed(12345);R=999;comb=c(magnetic, chemical);n=length(magnetic)
reps=numeric(R);t0=spearman_corr
for (i in 1:R) {
    pp=sample(comb,replace = FALSE)
    x1=pp[1:n]; y1=pp[-(1:n)]
    reps[i]=cor(x1, y1, method="spearman")
}
# 计算置换检验p值
p_value_perm=mean(abs(c(t0, reps)) >= abs(t0))
# 输出结果
cat(" Spearman rank相关系数为:", round(spearman_corr,3),"\n",
    "cor.test的p值:",signif(p_value_cor,3),"\n",
    "置换检验的p值:", round(p_value_perm,3),"\n")
```

### 结果分析：

$\hspace{1.5em}$斯皮尔曼秩相关检验结果为0.743，表明磁性和化学特性之间存在强正相关关系,即随着磁性值的增加，化学值也倾向于增加。cor.test给出的p值（约为1.85e-10）非常小，表明这种相关性在统计上是显著的，几乎可以排除偶然因素的影响。置换检验的p值为0.001，同样支持这一结论，进一步验证了原始数据的相关性是可靠的。

------

# 作业2024-10-28


## ***一、习题9.3***

$\hspace{1.5em}$使用Metropolis-Hastings算法从标准柯西分布中生成随机变量，丢弃链的前1000个样本，并比较生成的样本的分位数与标准柯西分布的分位数。进一步使用Gelman-Rubin方法评估链的收敛性。

$\hspace{1.5em}$标准柯西分布是一种具有无限方差的连续概率分布，其概率密度函数为
$$
 f(x) = \frac{1}{\pi \theta (1 + (x/\theta)^2)} ,
$$
其中 $\theta = 1$ 和 $\eta = 0$为标准柯西分布的参数。

#### 算法思路：

1. **定义目标分布**：在本例中为标准柯西分布。

2. **定义提议分布**：使用正态分布作为提议分布，其简单且容易生成随机样本。

3. **实现Metropolis-Hastings算法**：

   - 初始化链的首个值。
   
   - 迭代生成提议值并计算接受概率。
   
   - 根据接受概率决定是否接受提议值。

4. **生成直方图和潜在尺度缩减因子图**：绘制直方图比较生成样本与理论分布的符合程度，并使用Gelman-Rubin评估链的收敛性。

### 代码实现与结果分析：

```{r}
rm(list=ls())
## 柯西分布
theta = 1; eta = 0
df = function(x) {
  1/(theta*pi*(1+((x-eta)/theta)^2))}
## 正态分布-Proposal distribution
dg = function(x, df) {
  dnorm(x = x, mean = df) }
# Proposal分布中生成随机变量
rg = function(df) {
  rnorm(n = 1, mean = df) }
## M-H
mh = function (N, df, dg, rg) {
  x = numeric(N); k = 0
  x[1] = rg(1) # 从Proposal中产生X1
  u = runif(N)
  for (i in 2:N) {
    xt = x[i-1]; y = rg(xt)
    r = df(y)*dg(xt, y)/(df(xt)*dg(y, xt)) # 计算接受概率
    if (u[i] <= r) {
      #均匀随机数<=接受概率时接受
      x[i] = y } 
    else { 
      # 否则拒绝候选值，保留当前值
      x[i] = xt ; k = k + 1 }
  }
  print(round(k/N,3)); return(x) # 返回生成的随机变量序列
}
## 执行M-H算法
set.seed(1234); n = 20000; k = 3 # 链数
X = matrix(nrow = k, ncol = n)
for (i in 1:k) {
  X[i, ] = mh(n, df, dg, rg)
}
```

$\hspace{1.5em}$打印出的三条链的拒绝比例约为0.23，表明每条链中有23%的提议被拒绝，这表明提议分布与目标分布间有合理的平衡。

```{r}
#绘制直方图
x=X; hist(x, probability = TRUE, breaks = 100,col="skyblue")
plot.x = seq(min(x), max(x), 0.01);lines(plot.x, df(plot.x),col="deeppink")
# 绘制QQ图
sample_quantiles = t(apply(X, 1, quantile, probs = seq(0, 1, by = 0.1)))# 样本分位数
theoretical_quantiles = qcauchy(seq(0, 1, by = 1/n), 1, lower.tail = FALSE)#理论分位数
qqplot(theoretical_quantiles, sample_quantiles, main = "QQ plot")
abline(1, 2, col="deeppink")
```

$\hspace{1.5em}$从**直方图**可以看出，样本分布与理论柯西分布形状基本一致，尽管存在些许偏差，整体符合度较高。**QQ图**上大部分点都分布在红线附近，说明样本分位数与理论分位数非常接近，然而，尾部的一些点稍微偏离红线，可能暗示样本分布的尾部与理论分布相比有些许差异。表明样本与理论分布之间很好的一致性，Metropolis-Hastings采样器可能有效地从目标分布中生成了样本。

```{r}
## 计算潜在尺度缩减因子
Gelman.Rubin = function(phi) {
  phi = as.matrix(phi)
  k = nrow(phi); n = ncol(phi)
  phi.means = rowMeans(phi)
  B = n * var(phi.means)
  phi.w = apply(phi, 1, var)
  W = mean(phi.w)
  v.hat = W * (n - 1) / n + B / n
  r.hat = v.hat / W
  return(r.hat)
}
## 绘制累积均值(phi)图
b=1000; phi = t(apply(X, 1, cumsum))
for (i in 1:k) {
  phi[i,] = phi[i,] / (1:ncol(phi))
}
for (i in 1:k) {
  if (i == 1) {
    plot((b+1):n, phi[i,(b+1):n], ylim = c(-4, 2),
         type = "l", xlab = 'Index', ylab = bquote(phi))
  } else { lines((b+1):n, phi[i, (b+1):n], col = i) }
}
```

$\hspace{1.5em}$累积均值图展示了三条链的累积均值随迭代次数的变化情况。所有链的累积均值随迭代次数增加而趋于稳定，表明链可能已经收敛。初期的波动可能是由于链的burn-in period，在这段时间里，链还未达到其长期行为。随着迭代次数的增加，累积均值趋于平稳，这表明链已经混合良好。

```{r}
## 绘制潜在尺度缩减因子图
rhat = rep(0, n)
for (j in (b+1):n) {
  rhat[j] = Gelman.Rubin(phi[, 1:j])
}
plot(rhat[(b+1):n], type = "l", xlab = "", ylab = "R",
     ylim = c(1, round(max(rhat[(b+1):n]),digits = 0)))
abline(h = 1.2, lty = 2,col='red')
```

$\hspace{1.5em}$显示了潜在尺度缩减因子（Rhat）随迭代次数的变化，Rhat值是Gelman-Rubin统计量，比较了不同链的方差与链内方差。由图可以看出，初期Rhat值迅速下降，表明链在初期快速混合，但未达到平稳分布。在大约1000次迭代后低于1.2，这表明链混合良好。Rhat值的下降和稳定趋势表明随着迭代次数的增加，链逐渐收敛到其长期分布，即目标分布。

---

## ***二、习题9.8***

$\hspace{1.5em}$使用Gibbs抽样方法生成一个二元随机样本序列，并使用Gelman-Rubin方法检验样本序列的收敛性。。给定的联合密度函数为 \(f(x, y) = \frac{n}{x}yx+a-1(1 - y)^{n-x+b-1}\)\)，其中 \(x = 0, 1, \ldots; y \leq 0 \leq 1\)。对于固定的参数 \(a\), \(b\), \(n\)\)，条件分布分别是二项式分布（Binomial(n, y)）和Beta分布（Beta(x + a, n - x + b)）。

#### 算法思路：

1. 定义目标联合密度函数 \(f(x, y)\)。

2. 使用Gibbs抽样循环生成样本序列，循环中从条件分布中抽样更新样本值。

3. 绘制样本序列的散点图以可视化样本分布；计算样本序列的累积均值并绘制累积均值图。

4. 使用Gelman-Rubin方法检验样本序列的收敛性，Rhat值接近1.2通常表示样本序列收敛。

### 代码实现与结果分析：

```{r}
rm(list=ls())
# 定义目标联合密度函数
n = 1000;a = 30;b = 60
df = function (x, y) {
  gamma(n+1)/(gamma(x+1)*gamma(n-x+1))*y^(x+a-1)*(1-y)^(n-x+b-1)}
m = 20000 ;burnin = 1000;set.seed(12345)
## Gibbs抽样循环
x = matrix(0, nrow = m, 2)
for (i in 2:m) {
  xt = x[i-1,]  # 获取当前迭代的样本值
  xt[1] = rbinom(1, n, xt[2])  # 从条件分布Binomial(n, y)中抽样
  xt[2] = rbeta(1, xt[1] + a, n - xt[1] + b)  # 从条件分布Beta(x + a, n - x + b)中抽样
  x[i,] = xt  # 更新样本值
}
plot(x[,1], x[,2], xlab = bquote(X[1]), ylab = bquote(X[2]),
     main = "", cex = 0.5, ylim = range(x[,2]))
```

$\hspace{1.5em}$散点图显示了二元随机样本序列的分布情况,图中点的分布围绕某个区域密集分布，这表明样本序列可能很好地从目标分布中抽样。

```{r}
## 绘制累积均值(phi)图
phi = t(t(apply(x, 1, cumsum)))
for (i in 1:nrow(phi)) {
  phi[i,] = phi[i,] / (1:ncol(phi))}
plot((burnin+1):m, phi[i, (burnin+1):m],
      type = "l", xlab = 'Index', ylab = bquote(phi))
# 计算潜在尺度缩减因子
Gelman.Rubin = function(phi) {
  phi = as.matrix(phi);  k = nrow(phi); n = ncol(phi)
  phi.means = rowMeans(phi); B = n * var(phi.means);
  phi.w = apply(phi, 1, var);  W = mean(phi.w)
  v.hat = W*(n-1)/n+B/n; r.hat = v.hat / W
  return(r.hat)
}
## 绘制潜在尺度缩减因子图
rhat = rep(0, n)
for (j in (b+1):n) {
  rhat[j] = Gelman.Rubin(phi[, 1:j])}
plot(rhat[(b+1):n], type = "l", xlab = "", ylab = "R")
abline(h = 1.2, lty = 2)
```

$\hspace{1.5em}$累积均值图显示累积均值在初期有较大的波动，然后随着迭代次数增加逐渐趋于稳定；潜在尺度缩减因子图中，Rhat值在初期迅速上升并趋于稳定，且一直保持在1.2以下，这表明链可能已经收敛。

---

## ***三、证明题***

$\hspace{1.5em}$为证明给定MCMC的平稳性，需验证$K(s,r)f(s)=K(r,s)f(r)$,即如果从状态$s$开始，经过转移核$K$后，得到的分布和从状态$r$开始，经过转移核$K$后得到的分布相同，那么这个分布$f$就是平稳分布。


### 转移核的定义：

$$
K(r, s) = 1(s \neq r) \alpha(r, s) g(s \mid r) + 1(s = r) \left[1 - \int \alpha(r, s) g(s \mid r) ds\right]
$$


详细平衡条件要求对于所有的 $s$和 $r$，$K(s, r) f(s) = K(r, s) f(r)$必须成立.

### 证明：

1. 当 $s \neq r$时：

$\hspace{1.5em}$此时，转移核的表达式为：
   $$
   K(s, r) = \alpha(s, r) g(r \mid s)
   $$
    $$
   K(r, s) = \alpha(r, s) g(s \mid r)
    $$
其中接受概率的定义为：
    $$
   \alpha(s, r) = \min\left\{\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)}, 1\right\}
    $$
    $$
   \alpha(r, s) = \min\left\{\frac{f(s) g(r \mid s)}{f(r) g(s \mid r)}, 1\right\}
    $$

$\hspace{1.5em}$下证$\alpha(s, r) g(r \mid s) f(s) = \alpha(r, s) g(s \mid r) f(r)$.从左侧出发有：

   $$
   \alpha(s, r) g(r \mid s) f(s) = \min\left\{1, \frac{f(r) g(s \mid r)}{f(s) g(r \mid s)}\right\} g(r \mid s) f(s)
   $$
   $$
   = \frac{f(r) g(s \mid r)}{f(s) g(r \mid s)} g(r \mid s) f(s) \quad \text{(当 }\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)} < 1\text{)}= f(r) g(s \mid r)
   $$
   当 $\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)} \geq 1$ 时，左侧为$g(r \mid s) f(s)$

$\hspace{1.5em}$右侧：
   $$
   \alpha(r, s) g(s \mid r) f(r) = \min\left\{1, \frac{f(s) g(r \mid s)}{f(r) g(s \mid r)}\right\} g(s \mid r) f(r)
  $$
  $$
   = \frac{f(s) g(r \mid s)}{f(r) g(s \mid r)} g(s \mid r) f(r) \quad \text{(当 }\frac{f(s) g(r \mid s)}{f(r) g(s \mid r)} < 1\text{)}   = f(s) g(r \mid s)
   $$

   当 $\frac{f(s) g(r \mid s)}{f(r) g(s \mid r)} \geq 1$ 时，右侧为：$g(s \mid r) f(r)$,左侧和右侧的相等关系成立。
   
$\hspace{1.5em}$代入详细平衡条件，得到：

  $$
   \alpha(s, r) g(r \mid s) f(s) = \alpha(r, s) g(s \mid r) f(r)
   $$
故$K(s, r) f(s) = \alpha(s, r) g(r \mid s) f(s) = \alpha(r, s) g(s \mid r) f(r) = K(r, s) f(r)$成立。

2. **当 \(s = r\) 时**：

   此时，转移核为：

   $$
   K(s, s) = 1 - \int \alpha(s, s) g(s \mid s) ds
   $$

   由于 \(s = r\)，我们可以写出：

   $$
   K(s, s) f(s) = \left[1 - \int \alpha(s, s) g(s \mid s) ds\right] f(s)
   $$
   $$
   K(r, r) f(r) = \left[1 - \int \alpha(r, r) g(r \mid r) dr\right] f(r)
   $$

   由于 \(s = r\)，所以 \(K(s, s) = K(r, r)\) 且 \(f(s) = f(r)\)，这意味着：

  $$
   K(s, s) f(s) = K(r, r) f(r)
 $$

即对于所有可能的 \(s\) 和 \(r\)（无论是 \(s \neq r\) 还是 \(s = r\)），都有 \(K(s, r) f(s) = K(r, s) f(r)\)。这证明了目标分布 \(f\) 是给定 MCMC 算法的平稳分布。

------

#作业2024-11-04

## ***一、习题11.3***

### 问题重述

$\hspace{1.5em}$要求完成以下任务：

(a) 编写一个函数来计算给定数学表达式中的第$k$项，表达式为：

$$
\sum_{k=0}^{\infty}\frac{(-1)^k}{k! 2^k}\frac{\|a\|^{2 k+2}}{(2 k+1)(2 k+2)}\frac{\Gamma\left(\frac{d+1}{2}\right)\Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)},
$$

其中$d\geq1$是一个整数，$a$是$\mathbb{R}^d$中的一个向量$\|\cdot\|$表示欧几里得范数。要求进行适当的算术运算，以便可以计算出任意大的$k$和$d$的系数。

(b) 修改函数，使其计算并返回总和。

(c) 当$a = (1, 2)^T$时，计算。

### 问题分析

$\hspace{1.5em}$本题要求我们计算一个无穷级数的和，涉及到一些函数如伽玛函数、指数运算，当涉及到大数或小数时，在计算机上直接计算这些表达式可能会遇到数值稳定性问题。常采用**取对数**的方式解决，将乘法和除法转换为加法和减法，从而避免数值溢出或下溢。即：

$$
e^{(-1)^k[(2k+2)\ln(\|a\|)+\ln(\Gamma(\frac{d+1}{2}))+\ln(\Gamma(k+\frac{3}{2}))-\ln(k!)-k\ln(2)
-\ln((2k+1)(2k+2))-\ln(\Gamma(k+\frac{d}{2}+1))]}
$$      
      
### 代码实现

```{r}
rm(list=ls())
## (a) 编写计算第k项的函数
calcu_kterm = function(a, k, d) {
  #对原式取对数--这里有一个log(k!)在0的位置需要定义一下
  if (k==0) log_factorial=0 else log_factorial=sum(log(1:k))
  log_term = 0+(2*k+2)*log(sum(a^2))+log(gamma((d+1)/2))+log(gamma(k+3/2))-
      log_factorial-k*log(2)-log((2*k+1)*(2*k+2))-log(gamma(k+d/2+1))
  term = ((-1)^k)*exp(log_term) # 还原
  return(term)
}
## (b) 计算并返回总和
# 最大计算次数为max_iter 当计算小于1e-10时停止计算
getSum = function(a,d,max_iter=1000,tol=1e-10){ 
  sum_value = 0; Final = calcu_kterm(a, 0, d)
  exp_sum_value = Final
  for (k in 1:max_iter) {
    current_term = calcu_kterm(a, k, d)
    sum_value = sum_value + current_term
    if (abs(current_term) < tol){
      break}
  }
  return(sum_value)
}
## (c) Evaluate the sum
a = c(1, 2) ; d = length(a)
result = getSum(a, d)
cat("计算结果约为:",result)
absolute_values <- sapply(0:40, calcu_kterm, a = a, d = d)
plot(1:40, sapply(1:40, calcu_kterm, a = a, d = d), type = "l", 
     main = "级数项随k的变化",xlab = "k",ylab="", col = "deeppink")
```

$\hspace{1.5em}$由图可以验证，随着k增大，级数不断收敛。

---

## ***二、习题11.5***

### 问题重述

$\hspace{1.5em}$求解方程：

$$
\frac{2\Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)}\Gamma\left(\frac{k-1}{2}\right)}\int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k/2} du = \frac{2\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k}\Gamma\left(\frac{k}{2}\right)}\int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1)/2} du
$$

其中，$c_k$定义为：

$$
c_k = \sqrt{\frac{a^2 k}{k+1-a^2}}
$$

### 问题分析

$\hspace{1.5em}$方程的左侧和右侧都是关于k的函数，包含积分和伽玛函数。

 - 由于积分没有解析解，我们需要使用**数值积分**方法来近似计算积分的值，可以借助**integrate函数**使用自适应方法估计。

 - 进一步地，需要找到一个a,使得方程两侧的值相等,转换为方程的**求根问题**，采用**Brent算法**求解。

### 代码实现

```{r}
rm(list=ls())
solve.equation = function(k) {
  # 定义方程的左右两边
  term = function(n, a) {
    integral = function(u) (1+u^2/(n-1))^(-n/2)
    ck = sqrt(a^2*n/(n+1-a^2)) 
    p = 2/sqrt(pi*(n-1))*exp(lgamma(n/2)-lgamma((n-1)/2))
    return(p*integrate(integral, 0, ck)$value)  # 计算积分并返回
  }
  f = function(a){
    term(k, a)-term(k+1, a)}
  if (f(1.5)*f(2) < 0) {
    r = uniroot(f, c(0.5, 2))$root  # 求解方程
  } else r = NA #出现NA则通过绘制k对应图像调整区间
  return(r)
}
# 不同的k值求解
options(digits=4)
rs = sapply(c(4:25,100,500,1000), solve.equation)
print(rs)
```
---

## ***三***

### 问题重述

$\hspace{1.5em}$假设 $T_1,\ldots,T_n$是从期望为$\lambda$的指数分布中独立同分抽取的样本。由于右删失，大于$\tau$的值不会被观察到，因此观察到的值是$Y_i=T_iI(T_i\leq\tau)+\tau I(T_i>\tau)$,$i=1,\ldots,n$。假设 $\tau=1$，并且观察到的$Y_i$值如下：

$0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85$

使用E-M估计 $\lambda$，并与观察数据的最大似然估计（MLE）结果进行比较。

### 问题分析

##### 步骤 1: 定义混合分布

混合分布由两部分组成：

1. $T_i \leq \tau$ 时，$Y_i = T_i$，其概率密度函数为 $\lambda e^{-\lambda T_i}$。

2. $T_i > \tau$ 时，$Y_i = \tau$。

##### 步骤 2:  E-M 算法


- E-step：计算每个观测值$Y_i$来自指数分布的部分的期望值，即$Z_i = P(T_i \leq \tau | Y_i, \lambda)$。

- M-step：更新$\lambda$的估计值，使得对数似然函数最大化。

### 代码实现

$\hspace{1.5em}$根据公式 $E[X | X > \theta] = \frac{1}{\lambda} + \theta$，根据当前的$\lambda$和$\theta$计算调整后的条件期望值。


```{r}
rm(list=ls())
Y=c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau=1.0  # 删失阈值
# EM算法
EM=function(Y, tau, max_iter=100,tol=1e-6) {
  n=length(Y); lamb=1
  for (j in 1:max_iter) {
    expect=ifelse(Y<tau, Y, tau+1/lamb)  # E步
    new_lamb=1 / mean(expect)  # M步
    if (abs(new_lamb-lamb)<tol) {# 收敛判定
      break}
    lamb=new_lamb
  }
  return(lamb)
}
# 调用EM算法
lambda_EM=EM(Y, tau)
cat("EM算法估计的lambda：", lambda_EM, "\n")
```

$\hspace{1.5em}$最大似然估计（MLE）可以通过直接最大化似然函数来实现。对于截断数据，似然函数可以写为：
$$
L(\lambda) = \prod_{i=1}^n \left( \lambda e^{-\lambda Y_i} \right)^{Z_i} \left( e^{-\lambda \tau} \right)^{1-Z_i} 
$$

```{r}
# 负对数似然函数
neg_log_likelihood <- function(lambda, Y, tau) {
  # 对应删失和非删失数据的似然项
  term1 <- sum(log(lambda) - lambda * Y[Y < tau])  # 未删失数据
  term2 <- sum(log(1 - exp(-lambda * tau)) * (Y >= tau))  # 删失数据
  return(-(term1 + term2))
}
# MLE估计
MLE_lambda <- function(Y, tau) {
  # 最小化负对数似然
  result <- optimize(neg_log_likelihood, interval = c(0.0001, 10), Y = Y, tau = tau)
  return(result$minimum)
}
# 运行MLE估计
lambda_MLE <- MLE_lambda(Y, tau)
cat("最大似然估计的lambda：", lambda_MLE, "\n")
```

------

# 作业2024-11-11

## ***一、习题11.7***

$\hspace{1.5em}$最小化目标函数 \(4x+2y+9z\)，受以下约束条件限制：

1. \(2x+y+z \leq 2\)

2. \(x-y+3z \leq 3\)

3. \(x \geq 0\), \(y \geq 0\), \(z \geq 0\)

#### 代码实现

$\hspace{1.5em}$线性规划问题可以使用`lpSolve`包解决。

```{r}
rm(list=ls())
library(lpSolve)
# 目标函数系数(min 4x+2y+9z)
objective=c(4, 2, 9)
# 约束条件
constr=rbind(
  c(2, 1, 1), #  2x+y+z <= 2
  c(1, -1, 3) #  x-y+3z <= 3
)
direction=c("<=", "<=")
rhs=c(2, 3)
# 定义变量的下界(x, y, z >= 0)
lower.bounds=c(0, 0, 0)
# 使用lp函数求解线性规划问题
result=lp("min", objective, constr, direction, rhs,all.int=FALSE)
# 输出结果
result$solution
```

#### 结果验证

$\hspace{1.5em}$输出结果为[`r result$solution`]，这个解是一个正确的最优解，它符合所有约束条件并最小化了目标函数。

$\hspace{1.5em}$从目标函数\(4x+2y+9z\)来看，所有的系数都是正数(4、2、9)。因此，最优解会倾向于将 \(x\)、\(y\) 和 \(z\) 的值尽可能小，以便最小化目标函数。


1. 对第一个约束 \(2*0+0+0=0 \leq 2\) 是满足的。

2. 对第二个约束 \(0-0+3*0=0 \leq 3\) 也是满足的。

这表明 \(x=0\), \(y=0\), \(z=0\) 是一个可行解，并且目标函数的值为：

$$
4*0+2*0+9*0=0
$$

由于目标函数值为零，并且没有负值的系数，因此这就是最小值。

```{r,echo=FALSE}
rm(list=ls())
```

---

## ***二、Exercises 3, 4, 5***

### ***Exercises 3***

$\hspace{1.5em}$分别使用循环和lapply()函数，根据存储在这个列表中的公式来拟合mtcars数据集的线性模型：

```{r}
formulas=list(
  mpg ~ disp,
  mpg ~ I(1/disp),
  mpg ~ disp+wt,
  mpg ~ I(1/disp)+wt)
```

#### 代码实现

```{r}
# 加载数据集和所需包(microbenchmark测量两者的执行时间)
data(mtcars);library(microbenchmark)
## 一、使用 for 循环
time_for1=microbenchmark(
  for_loop={mod_for1=list()
    for(i in 1:length(formulas)) {
      mod_for1[[i]]=lm(formulas[[i]], data=mtcars)
    }},  times=10)
# 打印模型2结果
print(summary(mod_for1[[2]]))
## 二、使用 lapply()
time_lapply1=microbenchmark(
  lapply_loop={
    mod_lapply1=lapply(formulas, function(f) lm(f, data=mtcars))
  },times=10 )
# 打印模型2结果
print(summary(mod_lapply1[[2]]))
## 对比二者运行效率
print(time_for1)
print(time_lapply1)
```

#### 结果解释

$\hspace{1.5em}$输出说明：

- min：最小执行时间；lq：第一四分位数；mean：平均执行时间；median：中位数执行时间

- uq：第三四分位数；max：最大执行时间；neval：测试执行次数(10 次)

$\hspace{1.5em}$通过打印结果可以看到，**两种方法在四种模型下的拟合结果都相同**。

$\hspace{1.5em}$从**运行效率**来看，lapply()比for循环更高效。这是因为它是向量化的，并且 R 的内部优化通常能使 lapply() 执行得更快。

### ***Exercises 4***

$\hspace{1.5em}$在不使用匿名函数的情况下，使用for循环和lapply()函数，对下面每个mtcarsbootstrap样本上拟合模型mpg~disp。

```{r}
bootstraps=lapply(1:10, function(i) {
  rows=sample(1:nrow(mtcars), replace=TRUE)
  mtcars[rows, ]})
```

#### 代码实现

```{r}
library(microbenchmark)
## 一、使用 for 循环
time_for2=microbenchmark(
  for_loop={
    mod_for2=list()
    for(i in 1:length(bootstraps)) {
      mod_for2[[i]]=lm(mpg ~ disp, data=bootstraps[[i]])}
    },times=10)
## 二、使用 lapply()
fit_model=function(bootstrap_data) {# 定义拟合模型的函数
  lm(mpg ~ disp, data=bootstrap_data)}
time_lapply2=microbenchmark(
  lapply_loop={mod_lapply2=lapply(bootstraps, fit_model)},times=10)
## 比较两个方法的第一个模型结果
identical(coef(mod_for2[[1]]),coef(mod_lapply2[[1]]))#输出为True即相同
## 打印比较效率
print(time_for2);print(time_lapply2)
```

#### 结果解释

$\hspace{1.5em}$输出说明，两种方法得到的结果相同，且lapply()明显快于for循环。

### ***Exercises 3***

$\hspace{1.5em}$对于前两个练习中的每个模型，使用下面的函数提取R²值：

```{r}
rsq=function(mod) summary(mod)$r.squared
```

#### 代码实现

```{r}
options(digits=3)
## 练习1 
R1_1=lapply(mod_for1,rsq);R1_2=lapply(mod_lapply1,rsq) #提取R²值
df1=data.frame(mod_for=unlist(R1_1),mod_lapply=unlist(R1_2))
knitr::kable(t(df1),caption="练习一中四个模型的结果")
## 练习2
R2_1=lapply(mod_for2,rsq);R2_2=lapply(mod_lapply2,rsq) #提取R²值
df2=data.frame(mod_for=unlist(R2_1),mod_lapply=unlist(R2_2))
knitr::kable(t(df2),caption="练习二中模型结果")
```

```{r,echo=FALSE}
rm(list=ls())
```

---

## ***三、Excecises 3 and 6***

### ***Exercises 3***

$\hspace{1.5em}$以下代码模拟了对非正态数据进行t检验的性能。使用sapply()和一个匿名函数从每次试验中提取p值。

```{r}
trials=replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify=FALSE)
```

- 额外挑战：通过直接使用[[来去掉匿名函数。

#### 代码实现

```{r}
## 使用 sapply() 和匿名函数提取每次试验中的 p 值
p_values1=sapply(trials, function(x) x$p.value)
```

$\hspace{1.5em}$sapply()将匿名函数应用于trials列表的每个元素，提取每个t.test结果中的p.value，并返回一个包含所有 p 值的向量。

```{r}
## 额外挑战：使用 sapply() 和 [[ 来提取 p.value
p_values2=sapply(trials, `[[`, "p.value")
```

$\hspace{1.5em}$`[[` 是一种通过名称访问列表中元素的方式，允许通过名称或索引提取元素。与使用 $ 的方式是等效的， [[ 灵活性更高。

### ***Exercises 6***

$\hspace{1.5em}$实现一个结合Map()和vapply()的函数，创建一个lapply()的变体，该变体并行地遍历所有输入，并将输出存储在向量(或矩阵)中。

#### 代码实现

```{r}
m_v_lapply=function(..., FUN) {
  args=list(...)  # 获取输入的列表或向量
  result=Map(FUN, args[[1]], args[[2]]) # 将所有输入的参数传入FUN
  if (length(result) > 0) { # 获取第一个元素的类型来推断 FUN.VALUE 类型
    first_value=result[[1]]
    if (is.numeric(first_value)) {
      result=vapply(result, function(x) x, FUN.VALUE=numeric(1))
    } else if (is.character(first_value)) {
      result=vapply(result, function(x) x, FUN.VALUE=character(1))
    } else if (is.logical(first_value)) {
      result=vapply(result, function(x) x, FUN.VALUE=logical(1))
    } else if (is.list(first_value)) {
      result=vapply(result, function(x) x, FUN.VALUE=list())
    } else {stop("返回类型不可识别")}
  }
  return(result)}
## 运行示例1：加法
# 定义一个简单的函数：两个输入的乘法
multi_fun=function(x, y) {
  return(x*y)}
input1=1:5; input2=6:10
output1=m_v_lapply(input1, input2, FUN=multi_fun)
print(output1)
```

- `...` 用来接收输入参数的列表，可以是多个向量或其他类型的输入。

- `FUN` 用来指定希望对每一对输入参数执行的操作。

```{r,echo=FALSE}
rm(list=ls())
```

---

## ***三、Excecise4 4-5***

### ***Exercises 4***

$\hspace{1.5em}$制作一个更快版本的chisq.test()函数，该函数仅在输入为两个无缺失值的数值向量时计算卡方检验统计量。

$\hspace{1.5em}$卡方检验(Pearson's Chi-squared test)用于检验两个分类变量是否独立。公式如下：

\[
\chi^2=\sum \frac{(O_i-E_i)^2}{E_i}
\]

其中：
- \( O_i \) 是观察频数(Observed Frequency)
- \( E_i \) 是期望频数(Expected Frequency)，计算公式为：
  
\[
E_i=\frac{(行的总和) \times (列的总和)}{总样本数}
\]


```{r}
chisq_test=function(x, y) {
  if (!is.numeric(x) || !is.numeric(y)) {# 输入是否为数值向量
    stop("两个输入都必须是数值向量")}
  if (length(x) != length(y)) {# 向量长度是否相等
    stop("两个向量的长度必须相同") }
   if (any(is.na(x)) || any(is.na(y))) {# 是否有缺失值
    stop("向量中不允许有缺失值")}
    c_table=table(x, y) # 创建列联表
    # 计算期望频数矩阵
    row_sum=rowSums(c_table);col_sum=colSums(c_table);total_sum=sum(c_table)
    expected=outer(row_sum, col_sum, "*")/total_sum
    # 卡方统计量
    chi_squared_stat=sum((c_table-expected)^2/expected)
    # 自由度、p值
    df=(nrow(c_table)-1)*(ncol(c_table)-1)
    p_value=1-pchisq(chi_squared_stat, df)
    # 返回结果
    return(list(chi_squared_stat=chi_squared_stat,df=df,p_value=p_value))
}
## 示例使用
x=c(10,15,20,25,30); y=c(15,10,25,20,50)
result1=chisq_test(x, y)
print(result1)
```

### ***Exercises 5***

$\hspace{1.5em}$对于两个无缺失值的整数向量输入的情况，制作一个更快版本的 table() 函数，并用它来加速上面的卡方检验。

#### 代码实现

```{r}
fast_table=function(x, y) {
  x_levels=as.integer(factor(x))
  y_levels=as.integer(factor(y))
  # 初始化空的频数表
  counts=matrix(0, nrow=length(unique(x)), ncol=length(unique(y)))
  # 填充频数表
  for (i in seq_along(x)) {
    counts[x_levels[i], y_levels[i]]=counts[x_levels[i], y_levels[i]]+1
  }
  return(counts)}
# 使用之前定义的 chisq_test 函数,修改列联表定义的部分，进行卡方检验
chisq_test_fast=function(x, y) {
     c_table=fast_table(x, y) # 创建列联表
    # 计算期望频数矩阵
    row_sum=rowSums(c_table);col_sum=colSums(c_table);total_sum=sum(c_table)
    expected=outer(row_sum, col_sum, "*")/total_sum
    # 卡方统计量
    chi_squared_stat=sum((c_table-expected)^2/expected)
    # 自由度、p值
    df=(nrow(c_table)-1)*(ncol(c_table)-1)
    p_value=1-pchisq(chi_squared_stat, df)
    # 返回结果
    return(list(chi_squared_stat=chi_squared_stat,df=df,p_value=p_value))
}
## 示例使用
x=c(10,15,20,25,30); y=c(15,10,25,20,50)
result2=chisq_test_fast(x, y)
print(result2)
## 对比运行效率
library(microbenchmark)
time1=microbenchmark(chisq_test(x, y),times=50);print(time1)
time2=microbenchmark(chisq_test_fast(x, y),times=50);print(time2)
```

$\hspace{1.5em}$可以看到，使用改进的`fast_table()`后，输出结果不变，而运行速度得到明显地提升。

------

# 作业2024-11-18

### 作业要求

$\hspace{1.5em}$根据Exercise_9.8写一个Rcpp函数，使用“**qqplot**”函数比较编写的Rcpp函数生成的随机数与R中相应函数生成的随机数。使用“**microbenchmark**”函数比较这两个函数的计算时间。

### 代码实现

#### 1. C++ function 

$\hspace{1.5em}$首先创建一个m行2列的矩阵存储样本，对其赋初始值后进行Gibbs抽样循环：从条件分布Binomial(n, y)抽样更新 x1，从条件分布Beta(x + a, n - x + b)抽样更新x2。

- 输入参数：样本数 m、burnin 样本数、n、a、b。

- 使用 R::rbinom 和 R::rbeta 来从二项式和 Beta 分布中抽样。

$\hspace{1.5em}$C++函数 **gibbs_sampler.cpp**具体定义如下：

```{r,eval=FALSE}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix gibbs_sampling(int m, int n, double a, double b) {
  NumericMatrix x(m, 2);
  double xt_1=0.5; 
  double xt_2=0.5;
  for (int i=1; i < m; ++i) {
    xt_1=R::rbinom(n, xt_2);
    xt_2=R::rbeta(xt_1 + a, n - xt_1 + b);
    x(i, 0)=xt_1;
    x(i, 1)=xt_2;
  }
  return x;
}
```

#### 2. Rcpp v.s. R 

$\hspace{1.5em}$首先导入定义的Rcpp函数和需要的包：Rcpp、microbenchmark。

```{r}
rm(list=ls())
library(Rcpp);library(microbenchmark)
## 载入Rcpp函数
sourceCpp('../vignettes/gibbs.cpp')
```

$\hspace{1.5em}$使用“**qqplot**”函数比较编写的Rcpp函数生成的随机数与R中相应函数生成的随机数。使用“**microbenchmark**”函数比较这计算时间。

```{r}
# 设置参数和种子--Beta分布的参数与Gibbs抽样过程的迭代次数m
n=1000;a=30;b=60;m=10000
set.seed(1234)
## R编写Gibbs抽样函数
R_gibbs_sampling=function(m, n, a, b) {
  x=matrix(0, nrow=m, ncol=2)
  for (i in 2:m) {
    xt=x[i-1,]  # 当前迭代的样本值
    xt[1]=rbinom(1, n, xt[2])   # Binomial(n,y)中抽样
    xt[2]=rbeta(1,xt[1]+a,n-xt[1]+b) # Beta(x+a,n-x+b)中抽样
    x[i,]=xt # 更新样本值
  }
  return(x)
}

## 使用Rcpp的Gibbs抽样
x_rcpp=gibbs_sampling(m, n, a, b)
## 使用R编写的Gibbs抽样
x_r=R_gibbs_sampling(m, n, a, b)
```

```{r}
## 在一张图上plot散点图
par(mfrow=c(1, 1))
plot(x_rcpp[,1], x_rcpp[,2], xlab = 'x1', ylab ='x2',main = "",
     cex = 1, ylim = range(x_rcpp[,2]),col = 'darkgray',pch = 1)
par(new=TRUE)
plot(x_r[,1], x_r[,2],cex = 0.5,xlab = '', ylab ='',main = "",
     xaxt = 'n',yaxt = 'n',col ='darkred',pch = 18)
legend("bottomright", legend = c("Rcpp Samples", "R Samples"),
       col = c("darkgray", "darkred"), pch = c(1, 18), cex = 0.8)
```

- 通过 **plot()**可以观察到Gibbs抽样生成的二元样本序列的分布情况。可以看到二者围绕某个区域密集分布，样本序列可能很好地从目标分布中抽样；同时二者分布形态相似。

```{r}
## QQ图比较
par(mfrow=c(1, 2))
qqplot(x_rcpp[,1], x_r[,1], main="X1的QQ图(Rcpp v.s. R)", 
       xlab="Rcpp-X1", ylab="R-X1",col = "darkgray")
abline(0, 1, col = "red")  # 添加y=x参考线
qqplot(x_rcpp[,2], x_r[,2], main="X2的QQ图(Rcpp v.s. R)", 
       xlab="Rcpp-X2", ylab="R-X2",col = "darkgray")
abline(0, 1, col = "red")  # 添加y=x参考线
```

- 通过**qqplot()**可以检验生成的样本是否符合预期，如果两者生成的随机数分布一致，点会接近于一条直线。从图可以看出，大部分点都紧密地围绕在对角线附近，仅在尾部区域可能存在一些偏差。这表明**Rcpp和R生成的样本序列的分布非常相似**。

```{r}
## 两种方法的计算时间-重复10次结果
ts=microbenchmark(
  Rcpp=gibbs_sampling(m,n,a,b),
  R=R_gibbs_sampling(m,n,a,b),
  times=10) 
knitr::kable(summary(ts))
```

- 通过 **microbenchmark()**可以比较二者实现性能，明显看到，通过C++代码实现的Rcpp***速度快了10倍左右**，通过编写Rcpp函数**大幅度提升了代码运行效率**。

#### 3. 回到Exercise9.8上--收敛性验证

```{r}
## 累积均值(phi)图
phi=function(x,c="darkred",y='n'){
  burnin=1000 # 不稳定的烧入期
  phi = t(t(apply(x, 1, cumsum)))
  for (i in 1:nrow(phi)) {
    phi[i,] = phi[i,] / (1:ncol(phi))}
  plot((burnin+1):m, phi[i, (burnin+1):m],
       type = "l", xlab = 'Index', ylab = bquote(phi),
       col=c,yaxt = y) 
}
phi(x_r,c="darkgray",y='s');par(new=TRUE);phi(x_rcpp)
```

- **累积均值图**：绘制累积均值图，用于观察样本序列是否趋于稳定。可以看到，**两种方法累积均值图显示样本序列都是趋于稳定的**，符合收敛性要求。

```{r}
## 绘制潜在尺度缩减因子图
rhat=function(x,c='darkred'){
  ## 计算潜在尺度缩减因子
  phi = t(t(apply(x, 1, cumsum)))
  Gelman.Rubin = function(phi) {
    phi = as.matrix(phi);  k = nrow(phi); n = ncol(phi)
    phi.means = rowMeans(phi); B = n * var(phi.means);
    phi.w = apply(phi, 1, var);  W = mean(phi.w)
    v.hat = W*(n-1)/n+B/n; r.hat = v.hat / W
    return(r.hat)
}
  rhat = rep(0, n)
  for (j in (b+1):n) {
    rhat[j] = Gelman.Rubin(phi[, 1:j])
    }
  plot(rhat[(b+1):n], type = "l", xlab = "", ylab = "R",col=c)
  abline(h = 1.2, lty = 2,col='red') # 小于1.2认为是收敛
}
par(mfrow=c(1, 2))
rhat(x_r,c="black");rhat(x_rcpp)
```

- **Gelman-Rubin 检验**：绘制Rhat值图，观察是否低于1.2，小于1.2认为样本序列已经收敛。**图中两种方法的Rhat值始终低于1.2，充分表明序列已经收敛**。




